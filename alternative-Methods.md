# Alternatives to Vision-Language Models like ViLT and ALBEF

[link](https://github.com/sallymmx/ActionCLIP)

